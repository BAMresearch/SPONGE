{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More cleanup and fine-tuning of tests.\n",
    "V.12: cleanup of v11, also moving to volume-weighted size distributions. \n",
    "V.13: Using an excel file input to set up the tests and test groups. Considering daemon functions... \n",
    "V.14: tests using Martin's coils. Not much fine-tuning remaining. \n",
    "\n",
    "Use with Anaconda3-4.4.0 env. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPONGE: the brute-force scattering pattern simulator for any 3D structure. \n",
    "======\n",
    "\n",
    "Introduction\n",
    "--\n",
    "\n",
    "The SPONGE will calculate the theoretical scattering pattern for any 3D structure using the Debye equation for point-pairs. The structure to be used is defined using the common STereo Lithography (STL) format, the standard for describing structures in 3D for FDM printers. \n",
    "\n",
    "STL describes only the tesselated surface of a volumetric object, and is available as an output format for most 3D drawing software packages. Its common nature means that structures can be easily drawn by the researcher using their favourite 3D drawing tool, after which this code can simulate the scattering pattern. The disadvantage of the format is that \"colours\" are not strictly supported, and so objects with components of varying scattering contrast cannot be described. The second disadvantage of STL is that no units are specified, and so we here assume that 1 STL unit is 1 nm. \n",
    "\n",
    "The underlying code uses the Debye equation, as defined in 1915 by P. Debye (doi: 10.1002/andp.19153510606). This is one of the most fundamental scattering equations, and is commonly used to describe the scattering from arrangements of spherical objects, for example for scattering pattern simulation of proteins or amorphous structures. Strictly speaking, however, finite dimensions of the constituent spherical objects are not required, and the equation can be generalized for distributions of infinitisimally small points within an object. \n",
    "\n",
    "The Debye equation for single point-pair-distances $d$ (in nm) is given as:\n",
    "\n",
    "$I(q) = 4 \\pi V \\int^\\infty_0 \\frac{\\sin(qd)}{qd} \\mathrm{d}d$\n",
    "\n",
    "with $I(q)$ the scattering intensity, V the volume of the object, $q$ the scattering vector in reciprocal nm. \n",
    "\n",
    "The Python implementation heavily relies on the provision of the Visualization Toolkit (VTK: Schroeder, Will; Martin, Ken; Lorensen, Bill (2006), The Visualization Toolkit (4th ed.), Kitware, ISBN 978-1-930934-19-1) library. This is an extensive open-source software system for 3D graphics, and contains the functions required to interpret the STL format, and to determine for a distribution of points in 3D space, whether the points lie inside or outside of the closed hull defined by the surface. With this functionality, we can obtain a set of randomly distributed points that lie within the object within a reasonable timeframe. \n",
    "\n",
    "Uncertainties on the simulated data are estimated through generation of a number of independent repetitions. The datapoints at the same $q$-value in these repetitions are averaged, and a standard error on the mean is then used as the uncertainty estimate. \n",
    "\n",
    "Accuracy of the simulation can be affected by two parameters: the number of points within the object, and the number of repetitions. A higher number of repetitions will increase the accuracy of the simulation, and, given their independent nature, incidentally allow for a high degree of parallelisation in the computation. Increasing the number of points within the object will improve the high-$q$ region of the simulated data (as more point-pairs are available with shorter distances), but increases the complexity of the computation by $O^2$, and increases the memory requirements. *CHECK USAGE OF On NOTATION*\n",
    "\n",
    "The implementation has been tested by simulating the scattering patterns of spheres, ellipsoids, and cylinders, with various aspect ratios for ellipsoids and cylinders. The simulated scattering patterns were analysed using the appropriate model in SASfit, and the resulting parameters were found to agree within expectation. \n",
    "\n",
    "\n",
    "Size distribution\n",
    "--\n",
    "The size distribution can be taken into account for isomorphic size distributions: where the entire shape scales uniformly in all dimensions. With this assumption, we can easily calculate the scattering pattern due to the strict relation between q and the overall dimension, exploiting the singular factor $qd$ in the Debye equation. \n",
    "\n",
    "This means that we only need to calculate: $I_d(q) = \\sum_{n_d} I(q \\frac{\\Delta d}{d}) P(\\Delta d / d) $, where $P(\\Delta d / d)$ is the size distribtion relative to the calculated dimension, normalized to an integral probability of 1. In order to calculate $I(q \\frac{\\Delta d}{d})$, we interpolate $I(q)$. \n",
    "\n",
    "In the SPONGE, the size distribution is implemented by randomly picking a scaling factor for each repetition from a Gaussian distribution using the specified parameters for mean (typically 1) and width. In effect, this is averaging over scattering patterns from multiple, differently \"sized\" contributions, each scaled based on a random number picker using the size distribution parameters set in the picker (this approximates reality, where random particles are being probed). In order to get a more equal representation in the scattering pattern, we're applying volume weighting to the scattering patterns. This means that the Gaussian size distribution parameters describe a volume-weighted distribution as opposed to a number-weighted distribution.  \n",
    "\n",
    "The size distributions have been tested by simulating scattering from polydisperse spheres with a Gaussian distribution standard deviation (width) of up to 0.5. These simulated scattering patterns have been analysed using the SASfit package, and the results found to match within expectation.\n",
    "\n",
    "\n",
    "Notes\n",
    "--\n",
    "\n",
    "Due to the VTK python bindings only being properly and/or easily installed in Anaconda, we are limited to that. \n",
    "\n",
    "We are following some of the instructions on https://pyscience.wordpress.com/2014/09/03/ipython-notebook-vtk/ with regards to importing objects in STL format, and finding out whether points are inside or outside a closed hull. We then use the Debye equation for points to calculate the scattering function. A second intensity calculation was performed using the p(r) function, but this was found to give less representative results in a less efficient way. The very first attempt using the chord-length distribution function (or at least one variant thereof) appeared to work, but failed for anisotropic shapes due to the non-uniformity of the distribtion in space and direction of the resulting chords. \n",
    "\n",
    "Slit smearing\n",
    "--\n",
    "We use the slit smearing calculation procedure as developed in a separate notebook, which can be found in: ~/Documents/BAM/Projects/Smearing/. We also use the implementation in McSAS as a guide. \n",
    "\n",
    "For the slit smearing implementation to work, we need the beam profile parameters. In this case, we approximate the beam shape as trapezoidal, and determine this using a least-squares fit. This has been done by Andreas for our profile, resulting in the full width of the top of 2 times 1.305(14) nm$^{-1}$, and the full width of the bottom of 2 times 2.786(11) nm$^{-1}$. \n",
    "\n",
    "The slit smearing is applied to the models in a very similar way as the size distribution, by convoluting the intensity with the smearing profile. \n",
    "\n",
    "*NOTE: SLIT SMEARING CONSIDERED FOR REMOVAL!! *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, scipy, os\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import h5py\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current directory should be the 3DScatter directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the modules we need for this calculation:\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# from imp2.tools.reBin import reBin\n",
    "codepath = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if not codepath in sys.path:\n",
    "    sys.path.append(codepath)\n",
    "import sponge\n",
    "\n",
    "# for progress bar:\n",
    "# from ipywidgets import FloatProgress\n",
    "# from IPython.display import display\n",
    "\n",
    "# for parallel processes:\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickPointsInMeshV2(mesh, nPoints = 1000): \n",
    "    # choose points within the boundaries. These points are then (double)checked whether \n",
    "    # they lie inside or outside the object. Points outside the object are discarded. \n",
    "    # this process is repeated until nPoints have been found inside. \n",
    "    \n",
    "    # Find the limits of the mesh:\n",
    "    mesh.ComputeBounds() # already done upon STL read\n",
    "    (xMin, xMax, yMin, yMax, zMin, zMax) = mesh.GetBounds()\n",
    "    # print(\"Limits: x: {}, {}, y: {}, {}, z: {}, {}\".format(xMin, xMax, yMin, yMax, zMin, zMax))\n",
    "\n",
    "    nFound = 0\n",
    "    pts = []\n",
    "    TPCoord = np.zeros([nPoints, 3]) # test block\n",
    "    # inCoord = np.zeros([nPoints, 3]) # final point set\n",
    "    while (nFound < nPoints):\n",
    "        # generate a block of points to test\n",
    "        TPCoord[:, 0] = np.random.uniform(low = xMin, high = xMax, size = nPoints)\n",
    "        TPCoord[:, 1] = np.random.uniform(low = yMin, high = yMax, size = nPoints)\n",
    "        TPCoord[:, 2] = np.random.uniform(low = zMin, high = zMax, size = nPoints)\n",
    "        # add to vPts object:\n",
    "        # version using vtk points object:\n",
    "\n",
    "        TPts = vtk.vtkPoints()\n",
    "        TPts.SetDataType(vtk.VTK_DOUBLE)\n",
    "        dummy = [TPts.InsertNextPoint([TPCoord[j, 0], TPCoord[j, 1], TPCoord[j, 2]]) for j in range(nPoints)]\n",
    "        chkPts = vtk.vtkPolyData()\n",
    "        chkPts.SetPoints(TPts)\n",
    "\n",
    "        # set up location checker, parts of this may be moved outside loop later:\n",
    "        sel = vtk.vtkSelectEnclosedPoints()\n",
    "        sel.SetInputData(chkPts)\n",
    "        sel.SetSurfaceData(mesh)\n",
    "        sel.CheckSurfaceOn()\n",
    "        sel.Update() # 85.5% of runtime (as expected)\n",
    "        \n",
    "        pointi = [] # new list\n",
    "        j = 0\n",
    "        while (nFound < nPoints) and (j < nPoints):\n",
    "            if sel.IsInside(j):\n",
    "                pointi.append(j)\n",
    "                nFound += 1\n",
    "            j+=1\n",
    "            \n",
    "        # add to final set:\n",
    "        [pts.append(chkPts.GetPoint(j)) for j in pointi]\n",
    "        # for j in pointi:\n",
    "        #     inCoord[j,:] = chkPts.GetPoint(j)\n",
    "        # print(\"{} points found of requested {}\".format(nFound, nPoints))\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. from https://documen.tician.de/pyopencl/\n",
    "# in case of ERROR: clGetPlatformIDs -1001 try this (for Linux)\n",
    "# https://stackoverflow.com/a/20336286\n",
    "import pyopencl as cl\n",
    "\n",
    "indent = \"    \"\n",
    "print(\"Available OpenCL platforms: \")\n",
    "print(indent, cl.get_platforms())\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "\n",
    "print(\"OpenCL devices available:\")\n",
    "for device in platform.get_devices():\n",
    "     print(indent, device)\n",
    "\n",
    "ctx = cl.Context([platform.get_devices()[0]]) # using the first device\n",
    "queue = cl.CommandQueue(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.float32 # change the global calculus precision here\n",
    "ndist = -1 # limit the number of distances processed, for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctype = { np.float32: \"float\", np.float64: \"double\" }\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "{dtype} sinc(const {dtype} x) {\n",
    "    if (fabs(x) < 1e-20) return 1.;\n",
    "    return sin(M_PI * x) / (M_PI * x);\n",
    "}\n",
    "__kernel void sum(\n",
    "    __global const {dtype} *q, __global const {dtype} *dist, __global {dtype} *intensity,\n",
    "    int lenDist, __global int *indices)\n",
    "{\n",
    "  int i_q = get_global_id(0);\n",
    "  {dtype} int_sum = 0;\n",
    "//  int_sum = q[i_q];\n",
    "  for(int i_d = 0; i_d < lenDist; i_d++) {\n",
    "      int_sum += sinc( (dist[i_d] * q[i_q]) / M_PI );\n",
    "  }\n",
    "  indices[i_q] = i_q;\n",
    "  intensity[i_q] = int_sum;\n",
    "}\n",
    "\"\"\".replace(\"{dtype}\", ctype[dtype])).build()\n",
    "\n",
    "def pointsToScatterOCL(q, points, *args, **kwargs):\n",
    "    points = np.array(points)\n",
    "    dist = scipy.spatial.distance.pdist(points, metric = \"euclidean\")\n",
    "    dist = dist[:ndist].astype(dtype)\n",
    "    q = q.astype(dtype)\n",
    "    print(\"dist:\", dist.shape, dist.dtype, \"q:\", q.shape, q.dtype, \"pts:\", len(points))\n",
    "    \n",
    "    # 99.8% runtime is spend in this line, probably due to the explicit loop\n",
    "#    I = [ 4 * np.pi * (np.sinc(dist * qval / np.pi)).sum() / points.size**2\n",
    "#         for qval in q]\n",
    "\n",
    "    # input data setup\n",
    "    mf = cl.mem_flags\n",
    "    q_cl = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = q)\n",
    "    dist_cl = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = dist)\n",
    "\n",
    "    # output data setup\n",
    "    intensity = np.empty_like(q)\n",
    "    intensity.fill(np.nan) # initialize as NaN\n",
    "    intensity_cl = cl.Buffer(ctx, mf.WRITE_ONLY, intensity.nbytes)\n",
    "    # for testing\n",
    "    indices = np.zeros((len(intensity)*2,), dtype = np.int32) - 1\n",
    "#    print(\"indices0:\", indices)\n",
    "    indices_cl = cl.Buffer(ctx, mf.WRITE_ONLY, indices.nbytes)\n",
    "\n",
    "    prg.sum(queue, intensity.shape, None, q_cl, dist_cl, intensity_cl, np.int32(len(dist)), indices_cl)\n",
    "\n",
    "    cl.enqueue_copy(queue, intensity, intensity_cl)\n",
    "    cl.enqueue_copy(queue, indices, indices_cl)\n",
    "#    print(\"indices1:\", indices)\n",
    "    intensity = 4. * np.pi * intensity / len(points)**2\n",
    "    print(\"intensity:\\n\", intensity)\n",
    "    return intensity\n",
    "\n",
    "# Check on CPU with Numpy:\n",
    "#res = np.sinc(a_np) + np.sin(b_np)\n",
    "#print(res_np - res)\n",
    "#print(np.linalg.norm(res_np - res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointsToScatter(q, points, memSave = False):\n",
    "    # calculate the distance matrix between points, using a fast scipy function. \n",
    "    # This scipy function returns only unique distances, so only one distance \n",
    "    # value is returned for point1-point2 and point2-point1 combinations. It also\n",
    "    # removes the zero distances between point1-point1. \n",
    "    # we then calculate the scattering using the Debye equation. \n",
    "\n",
    "    points = np.array(points)\n",
    "    dist = scipy.spatial.distance.pdist(points, metric = \"euclidean\")\n",
    "    dist = dist[:ndist]\n",
    "    print(\"dist:\", dist.shape, \"q:\", q.shape, \"pts:\", len(points))\n",
    "    #print(dist)\n",
    "    #print(q)\n",
    "    if not memSave:\n",
    "        inter = np.outer(np.abs(dist), q) # 10% of runtime\n",
    "        # definition of np.sinc contains an additional factor pi, so we divide by pi. \n",
    "        # I = 2 * (np.sinc(inter / np.pi)).sum(axis=0) / points.size**2\n",
    "        # prefactor should be 4 \\pi.. perhaps.\n",
    "        # 90% of runtime\n",
    "        I = 4 * np.pi * (np.sinc(inter / np.pi)).sum(axis=0) / len(points)**2\n",
    "    else:\n",
    "        I = np.empty(q.shape)\n",
    "        I.fill(np.nan) # initialize as nan\n",
    "        # 99.8% runtime is spend in this line, probably due to the explicit loop\n",
    "        I = [ 4 * np.pi * (np.sinc(dist * qval / np.pi)).sum() / len(points)**2\n",
    "             for qval in q]\n",
    "\n",
    "#    print(\"intensity:\\n\", np.array([(np.sinc(dist * qval / np.pi)).sum() for qval in q]))\n",
    "    print(\"intensity:\\n\", np.array(I))\n",
    "    return I # , dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleRun(parameters):\n",
    "    \"\"\" Starts a single calculation based on the provided parameters \"\"\"\n",
    "    q = np.logspace(\n",
    "        np.log10(parameters[\"qmin\"]),\n",
    "        np.log10(parameters[\"qmax\"]),\n",
    "        parameters[\"nq\"])\n",
    "    mesh = sponge.STLToPolydata(Path(parameters[\"projectDirectory\"], parameters[\"filename\"]).as_posix())\n",
    "\n",
    "    # profiling: 5% of runtime in pickPointsInMeshV2()\n",
    "    pts = pickPointsInMeshV2(mesh, parameters[\"npoints\"])\n",
    "    # make sure we don't get too negative. \n",
    "    scaler = -1.0\n",
    "    while (scaler < 0):\n",
    "        # scaling factor to apply to the shape\n",
    "        scaler = np.random.normal(loc = parameters[\"mu\"], scale = parameters[\"sigma\"])\n",
    "\n",
    "    # profiling: 95% of runtime in pointsToScatter()\n",
    "    I = pointsToScatter(q * scaler, pts, parameters[\"memsave\"])\n",
    "    I_ocl = pointsToScatterOCL(q * scaler, pts, parameters[\"memsave\"])\n",
    "    print(\"max rel. difference:\", np.abs((np.array(I)-np.array(I_ocl))/np.array(I)).max())\n",
    "    vol = sponge.polydataToMass(mesh) * scaler**3 # I think this is correct with the scaler\n",
    "    # print(\"Correcting for volume: {} nm^3\".format(vol))\n",
    "    # return I * vol **2\n",
    "    return I, vol\n",
    "    \n",
    "def SpongeRun(parameters, progressBar = True):\n",
    "    q = np.logspace(\n",
    "        np.log10(parameters[\"qmin\"]),\n",
    "        np.log10(parameters[\"qmax\"]),\n",
    "        parameters[\"nq\"])\n",
    "\n",
    "    Pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())\n",
    "    mapParam = [parameters for i in range(int(parameters[\"nrep\"]))]\n",
    "    #rawData = Pool.map(singleRun, mapParam)\n",
    "    rawData = [singleRun(parameters) for i in range(2)]\n",
    "#     print(1, rawData)\n",
    "#     print(2, rawData.shape)\n",
    "    Pool.close()\n",
    "    Pool.join()\n",
    "    \n",
    "    # pick apart intensities and volume outputs:\n",
    "    rawDataI = []\n",
    "    rawDataV = []\n",
    "    for item in rawData:\n",
    "        rawDataI.append(item[0] * item[1]) # volume-weighting\n",
    "        rawDataV.append(item[1])\n",
    "    rawDataI = np.array(rawDataI)\n",
    "    rawDataV = np.array(rawDataV)\n",
    "        \n",
    "    data = pandas.DataFrame({\"Q\": q}) \n",
    "    data[\"I\"] = rawDataI.mean(axis = 0) * rawDataV.mean() # second half of scaling, first half is done in I.\n",
    "    data[\"IError\"] = rawDataI.std(axis = 0, ddof = 1) / np.sqrt(rawDataI.shape[0]) * rawDataV.mean()\n",
    "    \n",
    "    if parameters[\"ofname\"] is not None:\n",
    "        data.to_csv(Path(parameters[\"projectDirectory\"],parameters[\"ofname\"]).as_posix(), header = False, sep = ';', index = False)\n",
    "\n",
    "    return {\"data\"      : data,\n",
    "           \"parameters\" : parameters}\n",
    "\n",
    "def qEstimator(parameters):\n",
    "    # Returns the q limits estimated from the distance matrix. Does not consider the scaling settings\n",
    "    mesh = sponge.STLToPolydata(Path(parameters[\"projectDirectory\"], parameters[\"filename\"]).as_posix())\n",
    "        \n",
    "    pts = sponge.pickPointsInMeshV2(mesh, parameters[\"npoints\"])\n",
    "    points = np.array(pts)\n",
    "    dist = scipy.spatial.distance.pdist(points, metric = \"euclidean\")\n",
    "    return np.pi/dist.min(), np.pi/dist.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def runTests(Tests = pandas.DataFrame(), start = 0, last = None, group = None):\n",
    "    resultDict = {}\n",
    "    if group is not None:\n",
    "        testindices = Tests[Tests.testgroup == group].index.tolist() # old: .values\n",
    "                \n",
    "    for testindex in testindices:\n",
    "        if last is not None and last+min(testindices) < testindex:\n",
    "            break\n",
    "        print(\"Testindex: {} of {}\".format(testindex-min(testindices),\n",
    "                                           len(testindices)), end = \"\")\n",
    "        tic = datetime.now()\n",
    "        param = Tests.loc[testindex].to_dict()\n",
    "        try:\n",
    "            del res\n",
    "        except NameError:\n",
    "            pass\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "        res = SpongeRun(param)\n",
    "        resultDict.update({testindex: res})\n",
    "        print(\" in {} (h:m:s.ms)\".format(datetime.now() - tic))\n",
    "    return resultDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RayleighSphere(q, RayleighR, vol = 1.):\n",
    "    result = pandas.DataFrame()\n",
    "    # q = resultsDict[0][\"data\"][\"Q\"]\n",
    "    f = 3. * (np.sin(q*RayleighR) - q*RayleighR * np.cos(q*RayleighR)) / ((q*RayleighR)**3.) # Rayleigh form factor\n",
    "    result[\"Q\"] = q\n",
    "    result[\"I\"] = f**2 * vol**2\n",
    "    result[\"IError\"] = 0.01 * result[\"I\"]\n",
    "    # VolR = 4./3. * np.pi * RayleighR\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Testing the sphere.\n",
    "-- \n",
    "\n",
    "Our first step is to verify that the code calculates as expected. This is done using our beloved friend: the sphere. This sphere is defined with a radius of 1 nm (a diameter of 2 nm). We compare this wiht the Rayleigh function for a sphere. \n",
    "\n",
    "The parameters are: \n",
    "  - qMin: the minimum q to calculate\n",
    "  - qMax: the maximum q to calculate, it calculates *nq* points over q within this range. \n",
    "  - nPoints: the number of points to find *within the object*, from which the point-distance matrix is determined\n",
    "  - nRep: the number of times the calculation is repeated for improving average intensities and estimating uncertainties\n",
    "  - nq: number of q points to calculate over. Default 100\n",
    "  - mu: mean of the size scaling distribution, default 1. \n",
    "  - sigma: size distribution Gaussian width\n",
    "  - memSave: Save memory and speed up the process by calculating each Q value separately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tests = pandas.DataFrame() # CLEARS TESTS!\n",
    "# !! WORK HERE !!\n",
    "# change settings in the following two lines to adjust to project\n",
    "projectDirectory = ''\n",
    "excelFilename = \"Input.xlsx\"\n",
    "\n",
    "# projectDirectory = os.path.normpath(projectDirectory)\n",
    "\n",
    "# set excel file path\n",
    "# kansas = os.getcwd()\n",
    "# os.chdir(projectDirectory)\n",
    "efName = Path(projectDirectory, excelFilename)\n",
    "Tests = pandas.read_excel(efName, skiprows = 1)\n",
    "Tests = Tests.dropna(axis = 0, how = \"all\") #remove empty rows for cleaning up. \n",
    "Tests.columns = Tests.columns.str.lower() # lower case column names only\n",
    "# os.chdir(kansas)\n",
    "# cast to the right datatypes:\n",
    "Tests = Tests.astype({\"npoints\":\"int\", \"nq\":\"int\", \"nrep\":\"int\", \"memsave\":\"bool\"})\n",
    "Tests[\"projectDirectory\"] = projectDirectory # add a project directory to all the entries\n",
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(Tests.iloc[0].to_dict())\n",
    "#resultDict = runTests(Tests, last = 0, group = \"shapes\")\n",
    "rep_for_profiling = 1\n",
    "%lprun -f pointsToScatterOCL [singleRun({'testgroup': 'shapes', 'filename': 'models/koalas/Figure_Koala_pentagonal_hexecontahedron.stl', 'qmin': 0.1, 'qmax': 40.0, 'nq': 100, 'nrep': 100, 'npoints': 1000, 'mu': 1.0, 'sigma': 0.01, 'memsave': True, 'ofname': 'simdata/Koala_pentagonal_hexecontahedron.csv', 'projectDirectory': ''}) for i in range(rep_for_profiling)]\n",
    "#singleRun({'testgroup': 'shapes', 'filename': 'models/koalas/Figure_Koala_pentagonal_hexecontahedron.stl', 'qmin': 0.1, 'qmax': 40.0, 'nq': 100, 'nrep': 100, 'npoints': 1000, 'mu': 1.0, 'sigma': 0.01, 'memsave': True, 'ofname': 'simdata/Koala_pentagonal_hexecontahedron.csv', 'projectDirectory': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12, 6])\n",
    "csl = list()\n",
    "ratio = list()\n",
    "#for key in Tests[Tests.testgroup == \"Martin's Helix\"].index.tolist():\n",
    "for key in Tests.index.tolist():\n",
    "    \n",
    "    sponge.simPlot(resultDict[key][\"data\"], title = Tests.loc[key][\"filename\"])\n",
    "\n",
    "plt.title(\"the datasets from the simulations\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlim(.1, 40)\n",
    "plt.ylim(1e-2, 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(data=resultDict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
